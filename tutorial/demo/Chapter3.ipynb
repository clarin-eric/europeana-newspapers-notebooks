{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae044b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbformat lxml\n",
    "\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28157ae-a393-4406-8381-c9c1921ca2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "from common import _check_task_size, data_dir, metadata_dir, set_id, output_file, unpack_metadata, unzip_file, zip_file\n",
    "# Getters\n",
    "from common import get_liner_output_files_ch3, get_date_from_metadata, get_resource_ids_from_metadata, get_resource_file, get_spellchecked_resources_ch3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76fdd3-6489-4f92-b590-b2e322e88683",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with Liner2\n",
    "\n",
    "In this section we will present how to use Europeana bibliographic resources with Named Entity Recognition (NER) tool for Polish using [lpmn_client](https://wiki.clarin-pl.eu/en/nlpws/lpmn_client) and [Liner2](https://github.com/CLARIN-PL/Liner2). Due to the server side limitation, we ensure 2M limit on size on the task with function defined below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c493718-6d21-475f-9886-54ce36ef31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Install lpmn client and import it\n",
    "\"\"\"\n",
    "!pip install -i https://pypi.clarin-pl.eu lpmn_client\n",
    "\n",
    "from lpmn_client import download_file, upload_file\n",
    "from lpmn_client import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25477234-dba1-4c90-86b5-d2bcf2cea200",
   "metadata": {},
   "source": [
    "### Spellchecking with lpmn client\n",
    "\n",
    "OCR output tends to contains spelling mistakes that adds noise to the data. Below we show how to use lpmn clinet to specify task pipeline in order to obtain spell-checked textual data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2d936-f059-411e-bcc5-4122a9cdaded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function for tasking lpmn client with Liner2 NER pipeline with task size control \n",
    "\"\"\"\n",
    "\n",
    "def lpmn_client_task(resources, task, names=[]):\n",
    "    \"\"\"\n",
    "        Wrap over CLARIN-PL lpmn client with control of the task size in order to avoid jamming the task queue on the server side\n",
    "        \n",
    "        :param list resources: list of paths to the resources to be processed\n",
    "        :param str task: string defining pipeline, e.g. \"speller2\" or \"\"\n",
    "        :param list names: optional list of names for output files, has to be same length as resources\n",
    "        :returns list: list of paths to the output zip files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Size check\n",
    "    _check_task_size(resources)\n",
    "    # Upload reasources to task queue\n",
    "    job_ids = [upload_file(resource_file) for resource_file in resources]\n",
    "    # Specify pipeline \n",
    "    t = Task(task)\n",
    "    # Run uploaded tasks with pipeline\n",
    "    output_file_ids = [t.run(job_id, verbose=True) for job_id in job_ids]\n",
    "    if names:\n",
    "        output = [download_file(output_file_id, output_file, f\"{filename}.zip\") \n",
    "                         for output_file_id, filename in zip(output_file_ids, names)]\n",
    "    else:\n",
    "        output = [download_file(output_file_id, output_file, f\"{os.path.basename(resource)}.zip\") \n",
    "                         for output_file_id, resource in zip(output_file_ids, resources)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf0eb0-9333-480f-9f08-985e16ced020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Let's select resources. We will use Izraelita newspapers from 1910-1913 to investigate most frequent Named Entities over the years on January. \n",
    "\"\"\"\n",
    "\n",
    "# Prepare metadata of the collection\n",
    "unpack_metadata(set_id, metadata_dir)\n",
    "izraelita_metadata_files = [f\"{metadata_dir}/Izraelita_1911.xml\",\n",
    "                            f\"{metadata_dir}/Izraelita_1912.xml\",\n",
    "                            f\"{metadata_dir}/Izraelita_1913.xml\",\n",
    "                           ]\n",
    "\n",
    "# Use getters to obtain date and id from metadata\n",
    "izraelita_metadata_trees = [etree.parse(izraelita_metadata_file) for izraelita_metadata_file in izraelita_metadata_files]\n",
    "ids = [get_resource_ids_from_metadata(izraelita_metadata_tree) for izraelita_metadata_tree in izraelita_metadata_trees]\n",
    "dates = [get_date_from_metadata(izraelita_metadata_tree) for izraelita_metadata_tree in izraelita_metadata_trees]\n",
    "\n",
    "# Get all issues from January\n",
    "ids_dates_january = []\n",
    "for _ids, _dates in zip (ids, dates):\n",
    "    year = []\n",
    "    for _id, date in zip(_ids, _dates):\n",
    "        if date.month==1:\n",
    "            year.append((_id, date))\n",
    "    ids_dates_january.append(year)\n",
    "\n",
    "# Map id to reasource\n",
    "resources_dates = [(get_resource_file(_id), date) for year in ids_dates_january for _id, date in year ]\n",
    "print(resources_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3407fbc-f163-454d-89ea-697a218a9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Let's spell check selected resources\n",
    "\"\"\"\n",
    "\n",
    "# Safety check of the size of all resources meant for the task\n",
    "_check_task_size([resource for resource, _ in resources_dates])\n",
    "\n",
    "# For offline run uncomment block below and comment last line\n",
    "# for resource, date in resources_dates:\n",
    "#     lpmn_client_task([resource], \"speller2\", [f\"{date}_speller\"])\n",
    "# speller2_output_file_paths = [unzip_file(f\"{output_file}/{date}_speller.zip\")[0] for _, date in resources_dates]\n",
    "\n",
    "speller2_output_file_paths = get_spellchecked_resources_ch3()\n",
    "speller2_output_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb3aef-e72e-4799-9f84-a5ce1491455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety check of the size of all resources meant for the task\n",
    "_check_task_size(speller2_output_file_paths)\n",
    "\n",
    "for resource, (_, date) in zip(speller2_output_file_paths, resources_dates):\n",
    "    lpmn_client_task([resource],'any2txt|wcrft2|liner2({\"model\":\"top9\"})', [f\"{date}_liner\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cad33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "liner2_output_file_paths = [unzip_file(f\"{output_file}/{date}_liner.zip\")[0] for _, date in resources_dates]\n",
    "liner2_output_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd0567-35b3-43e0-9eb1-68cf7938ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function for extracting annotations from Liner2 xml output\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def liner2_xml_to_annotation(xml_tree):\n",
    "    \"\"\"\n",
    "        Converts xml doc into list of annotations and tokens\n",
    "        \n",
    "        :param str path_to_xml: path to .xml Liner2 output file\n",
    "        :returns list: list of tuples (annotation_type, [tokens])\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = xml_tree.xpath(\"//sentence\")\n",
    "    annotated_tokens = [sentence.xpath(\"./tok[./ann!=0]\") for sentence in sentences]\n",
    "    # Prune empty lists\n",
    "    annotated_tokens = filter(lambda x: True if x else False, annotated_tokens)\n",
    "    annotated_tokens = [_chain_annotations(sentence) for sentence in annotated_tokens]\n",
    "    return annotated_tokens\n",
    "        \n",
    "def _chain_annotations(sentence: list):\n",
    "    annotation_heads = [token.xpath(\"./ann[@head]\") for token in sentence]\n",
    "    for token in annotation_heads:\n",
    "        for annotation_head in token:\n",
    "            annotation_channel = annotation_head.xpath(\"./text()\")[0]\n",
    "            annotation_type = annotation_head.get(\"chan\")\n",
    "            annotation_tokens = [token.xpath(\"./lex/base/text()\")[0] for token in sentence if token.xpath(f\"./ann[text()={annotation_channel}]\")]\n",
    "    return annotation_type, annotation_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb08a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Parse NER output and investigate 10 most common named entities by years\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "named_entities_counts = {}\n",
    "for _, date in resources_dates:\n",
    "    for root, _, files in os.walk(f\"{output_file}/{date}_liner\"):\n",
    "        for filename in files:\n",
    "            path_to_annotated_output = os.path.join(root, filename)\n",
    "            xml_tree = etree.parse(path_to_annotated_output)\n",
    "            # Get stats\n",
    "            token_nb = sum([1 for _ in xml_tree.xpath(\"//tok\")])\n",
    "            annotation_list = liner2_xml_to_annotation(xml_tree)\n",
    "            annotation_counts = Counter(f\"{annotation_type}|{' '.join(annotation_tokens)}\" for annotation_type, annotation_tokens in annotation_list)\n",
    "            if date.year in named_entities_counts.keys():\n",
    "                named_entities_counts[date.year].append((token_nb, annotation_counts))\n",
    "            else:\n",
    "                named_entities_counts[date.year] = [(token_nb, annotation_counts)]\n",
    "most_common_overall = {year: (sum([token_nb for token_nb, _ in named_entities_counts[year]]), \n",
    "                              sum([counter for _, counter in named_entities_counts[year]], Counter()).most_common(10)\n",
    "                             ) for year in named_entities_counts.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5f9ca-884f-4e6d-ac01-413c1beee0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
