{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d95b42d-f8ed-41af-a21b-53fd5aa96256",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 3: data filtering and running NLP tasks\n",
    "\n",
    "In this exercise you will first learn how to use metadata properties for filtering data. We will then apply this to an example NLP pipeline.\n",
    "\n",
    "----\n",
    "Like in the previous exercise, we first need to install and 'import' some packages. Run the following cell to get everything in place. Notice that this is a cell that might take a bit more time to run. While `[*]` is shown next to a cell, this means that it is being processed or waiting to be processed and has not yet completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51ea62-15a0-4df9-b095-029742393adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Install lxml and import it\n",
    "\"\"\"\n",
    "!pip install lxml\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "\"\"\"\n",
    "    Install lpmn client and import it\n",
    "\"\"\"\n",
    "!pip install -i https://pypi.clarin-pl.eu lpmn_client\n",
    "\n",
    "from lpmn_client import download_file, upload_file\n",
    "from lpmn_client import Task\n",
    "\n",
    "# We will also be using some core libraries\n",
    "from datetime import date\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Globals\n",
    "from common import align_resources, _check_task_size, data_dir, metadata_dir, nsmap, set_id, print_xml, output_file, unpack_metadata, unzip_file, zip_file\n",
    "# Getters\n",
    "from common import ex3_filter_by_date_and_content, get_date_from_metadata, get_resource_ids_from_metadata, get_resource_file, get_spellchecked_resources_ex3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a9151-6d39-42cf-95b7-aa02a680d1a9",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "For many research questions, we want to analyse one or more specific data segments based on some criteria. For this exercise we assume that we are interested to find names of persons, organisations, places etcetera found in texts published in the first week of World War I containing the phrase _w Polsce_ (_\"in Poland\"_).\n",
    "\n",
    "In the next cells, **create the list of file paths of resources that meet these criteria**:\n",
    "- publication date between 28 July 1914 and 11 August 1914\n",
    "- the resource file contains the text `w Polsce`\n",
    "\n",
    "We will start with some preparations, no need to change the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b33e8-c868-416f-84a1-e4011ddbe6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the metadata. We put this in its own cell so that we can run the filtering process separately\n",
    "set_metadata_dir = unpack_metadata(set_id, metadata_dir)\n",
    "\n",
    "set_metadata_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807cfe8-6f6d-4af6-9255-4662a31d1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide two helper functions that gets all issue identifiers and the associated dates out of\n",
    "# a metadata record. You can use this as is.\n",
    "\n",
    "def get_issues_id_and_date(metadata_tree):\n",
    "    \"\"\"\n",
    "        Returns a list of tuples (id, date) for all issues in the metadata tree.\n",
    "        The 'date' part is a date object that supports retrieval of the date parts, i.e.\n",
    "        `date.year`, `date.month`, `date.day`\n",
    "    \"\"\"\n",
    "    issue_descriptions = metadata_tree.xpath('//cmdp_text:SubresourceDescription', namespaces=nsmap)\n",
    "    issues = [get_id_and_date_from_description(description) for description in issue_descriptions  if description is not None]\n",
    "    return [issue for issue in issues if issue is not None]\n",
    "\n",
    "def get_id_and_date_from_description(description_element):\n",
    "    \"\"\"\n",
    "        Helper that gets the identifier and date for a single issue. Returns None\n",
    "        if there is identifier and date information are not both present.\n",
    "    \"\"\"\n",
    "    issue_ids = description_element.findall('./cmdp_text:IdentificationInfo/cmdp_text:identifier', namespaces=nsmap)\n",
    "    issue_dates_start = description_element.find('./cmdp_text:TemporalCoverage/cmdp_text:Start/cmdp_text:date', namespaces=nsmap)\n",
    "    if len(issue_ids) > 0 and issue_dates_start is not None:\n",
    "        for issue_id in issue_ids:\n",
    "            if issue_id.text.isnumeric():\n",
    "                return (issue_id.text, date.fromisoformat(issue_dates_start.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c73b15-af64-4546-aeba-bc1c976b019f",
   "metadata": {},
   "source": [
    "Now we have to **put everything together to filter the metadata and collect the files containing text for issues that match our criteria**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5360595-f56f-4d78-87e5-1b59f380fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for metadata_file in os.listdir(set_metadata_dir):\n",
    "    full_path = f'{set_metadata_dir}/{metadata_file}'\n",
    "    tree = etree.parse(full_path)\n",
    "    for info in get_issues_id_and_date(tree):\n",
    "        (issue_id, issue_date) = info\n",
    "        # We now have a numer identifier `issue_id` and a date `issue_date`\n",
    "        # from which we can get the year, month, day through `issue_date.year`,\n",
    "        # `issue_date.month`, `issue_date.day`. Use this to decide whether to include \n",
    "        # this issue.\n",
    "        \n",
    "        # If the date matches the desired range, we need to look at the resource itself\n",
    "        # to see if the target text appears.\n",
    "        # If you want, you can make use of the provided function get_resource_file(issue_id)\n",
    "        # to determine the path to the file.\n",
    "        #\n",
    "        # In Exercises set 2 we explored how to open a file and look for text inside\n",
    "        \n",
    "        # If both criteria match, we only need to add the file path to the array, which is\n",
    "        # done with `files.append(issue_id)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cefa6-f387-4f77-b3de-c5625141fc8f",
   "metadata": {},
   "source": [
    "In the next cell we **compare the result to a predefined solution**. In the following cells we will use the outcome of the predefined solution, so don't worry about moving on even if the outcomes do not fully match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3560b-c5aa-478d-9d1f-35af18925b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_result = files\n",
    "print(f'Number of files in our own result: {len(my_result)}')\n",
    "\n",
    "# now we run the predefined solution\n",
    "predefined_result = ex3_filter_by_date_and_content(set_metadata_dir,\n",
    "                                       date.fromisoformat('1914-07-28'), \n",
    "                                       date.fromisoformat('1914-08-11'), 'w Polsce')\n",
    "print(f'Number of files in result from predefined solution: {len(predefined_result)}')\n",
    "\n",
    "if len(my_result) == len(predefined_result):\n",
    "    print('The counts match!')\n",
    "else:\n",
    "    print('The counts do not match :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6fa08-48bf-4ac7-a7f7-6b887aaf16e0",
   "metadata": {},
   "source": [
    "## Exercise 3.2\n",
    "In this exercise we will try to investigate effect of using contemporary spellchecking on archival textual data. Due to the time required for spellchecking we provde a mapping to already processed files. Your job will be to **run the NER pipeline on raw and spell-corrected textual data, and compare number of tokens and found annotations**.\n",
    "\n",
    "First we define a function that calls the CLARIN-PL NLP service with a specific tasks for a set of resources. You don't have to change anything in the next cell, but read the code and try to understand what is happening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd3a97-7539-491d-beda-5698e4d78828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function for tasking lpmn client with Liner2 NER pipeline with task size control \n",
    "\"\"\"\n",
    "\n",
    "def lpmn_client_task(resources, task, names=[]):\n",
    "    \"\"\"\n",
    "        Wrap over CLARIN-PL lpmn client with control of the task size in order to avoid jamming the task queue on the server side\n",
    "        \n",
    "        :param list resources: list of paths to the resources to be processed\n",
    "        :param str task: string defining pipeline, e.g. \"speller2\" or \"\"\n",
    "        :param list names: optional list of names for output files, has to be same length as resources\n",
    "        :returns list: list of paths to the output zip files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Size check\n",
    "    _check_task_size(resources)\n",
    "    # Upload reasources to task queue\n",
    "    job_ids = [upload_file(resource_file) for resource_file in resources]\n",
    "    # Specify pipeline \n",
    "    t = Task(task)\n",
    "    # Run uploaded tasks with pipeline\n",
    "    output_file_ids = [t.run(job_id, verbose=True) for job_id in job_ids]\n",
    "\n",
    "    if names:\n",
    "        # Names were provided, use these when unpacking\n",
    "        output = [download_file(output_file_id, output_file, f\"{os.path.basename(filename)}.zip\") \n",
    "                         for output_file_id, filename in zip(output_file_ids, names)]\n",
    "    else:\n",
    "        output = [download_file(output_file_id, output_file, f\"{os.path.basename(resource)}.zip\") \n",
    "                         for output_file_id, resource in zip(output_file_ids, resources)]\n",
    "    return output\n",
    "\n",
    "resource_files_spellchecked = None\n",
    "resource_files_raw = predefined_result # using the output we know to be correct from exercise 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890d169-82d0-4a37-9e37-70c864f11dd4",
   "metadata": {},
   "source": [
    "Run the next cell to use pre-defined spellchecked resources if they are available to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994cda7f-8736-4a37-aedb-ffc364648cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get resources for the pipeline\n",
    "\"\"\"\n",
    "\n",
    "# If running outside workshop, this may not work in which case the next cell will cause the pipeline to run\n",
    "resource_files_spellchecked = get_spellchecked_resources_ex3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d73a0-64e4-4d4e-9c44-54934b0649c3",
   "metadata": {},
   "source": [
    "The next cell triggers a pipeline run to obtain spellchecked resources if necessary. If the predefined result is available, it will only print a message that running of the pipeline is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8a492-833f-4994-a499-8a4893d0ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resource_files_spellchecked:\n",
    "    print('Pre-defined resources found, skipping running of the pipeline.')\n",
    "else:\n",
    "    print('Pre-defined resources not found, will run the pipeline to obtain. This may take a while!')\n",
    "    resource_files_spellchecked = lpmn_client_task(resource_files_raw, \"speller2\")\n",
    "    resource_files_spellchecked = [unzip_file(r) for r in resource_files_spellchecked]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438fd1b2-fdd6-4098-893d-8313cc847b4c",
   "metadata": {},
   "source": [
    "In the next cell, **we must call the NLP service with a pipeline that includes the Liner2 NER tool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b123a-a3c4-4fd4-aeda-ef9332e295d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Run NER task on raw (un-spellchecked) input\n",
    "\"\"\"\n",
    "# Keep the following two lines as they are, you will need to pass these to the NLP pipeline\n",
    "pipeline = 'any2txt|wcrft2|liner2({\"model\":\"top9\"})'\n",
    "resource_files_raw_names = [f\"{os.path.basename(r)}_raw\" for r in resource_files_raw]\n",
    "\n",
    "# Uncomment the line below and complete it to run the pipeline\n",
    "# output_files_raw = lpmn_client_task(resource_files_raw, .............\n",
    "\n",
    "print(\"NER pipeline over raw resources finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a80b31-9ec8-471e-b91a-1f7a5955c04a",
   "metadata": {},
   "source": [
    "Now do the same thing for the spellchecked resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3299be-c91a-4b7c-9ce7-b0266f01f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Run NER task on spellchecked data\n",
    "\"\"\"\n",
    "\n",
    "pipeline = 'liner2({\"model\":\"top9\"})'\n",
    "resource_files_spellchecked_names = [f\"{os.path.basename(r)}_spellchecked\" for r in resource_files_spellchecked]\n",
    "\n",
    "# Complete this cell so that we will have a variable 'output_files_spellchecked' that allows us to compare \n",
    "# the NER output for raw and spellchecked data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a094bbc-43ee-45de-aa97-1bf58b24d841",
   "metadata": {},
   "source": [
    "Simply run the following cell for some required organisation of the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7ccca-8c27-4c96-8bbf-27fea52de70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Unpacking and file name changes in order to avoid clashes\n",
    "\"\"\"\n",
    "\n",
    "output_files_raw_fixed = [f\"{o.replace('home%jovyan%data%9200357%', '')}\" for o in output_files_raw]\n",
    "output_files_raw_fixed = [f\"{unzip_file(o)[0]}\" for o in output_files_raw_fixed]\n",
    "for o in output_files_raw_fixed:\n",
    "    os.rename(o, f\"{o.replace('home%jovyan%data%9200357%', '')}_raw\")\n",
    "\n",
    "output_files_raw_fixed = [f\"{o.replace('home%jovyan%data%9200357%', '')}_raw\" for o in output_files_raw_fixed]\n",
    "\n",
    "output_files_spellchecked_fixed = [f\"{o.replace('home%jovyan%data%9200357%', '')}\" for o in output_files_spellchecked]\n",
    "output_files_spellchecked_fixed = [f\"{unzip_file(o)[0]}\" for o in output_files_spellchecked_fixed]\n",
    "for o in output_files_spellchecked_fixed:\n",
    "    os.rename(o, f\"{o.replace('home%jovyan%data%9200357%', '')}_spellchecked\") \n",
    "    \n",
    "output_files_spellchecked_fixed = [f\"{o.replace('home%jovyan%data%9200357%', '')}_spellchecked\" for o in output_files_spellchecked_fixed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec908a6-9f89-470b-addf-ff0aec283ef0",
   "metadata": {},
   "source": [
    "## Exercise 3.3\n",
    "We will now **carry out some analysis** on the different output of the two runs (raw and spellchecked data). For this, we first define some functions that will make it easy to count tokens and annotations in the pipeline output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139e7f6-3a2b-43fb-ac28-ef6096c23e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Functions for parsing output and basic stats\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def count_tokens(ner_output_tree):\n",
    "    return sum([1 for _ in ner_output_tree.xpath(\"//tok\")])\n",
    "\n",
    "def list_annotations(ner_output_tree):\n",
    "    return liner2_xml_to_annotation(ner_output_tree)\n",
    "\n",
    "def count_annotations(ner_output_tree) -> Counter:\n",
    "    return Counter(f\"{annotation_type}|{' '.join(annotation_tokens)}\" for annotation_type, annotation_tokens in list_annotations(ner_output_tree))\n",
    "\n",
    "def liner2_xml_to_annotation(ner_output_tree):\n",
    "    \"\"\"\n",
    "        Converts xml doc into list of annotations and tokens\n",
    "        \n",
    "        :param ElementTree ner_output_tree: lxml instance of ET of NER output xml\n",
    "        :returns list: list of tuples (annotation_type, [tokens])\n",
    "    \"\"\"\n",
    "    sentences = ner_output_tree.xpath(\"//sentence\")\n",
    "    annotated_tokens = [sentence.xpath(\"./tok[./ann!=0]\") for sentence in sentences]\n",
    "    # Prune empty lists\n",
    "    annotated_tokens = [annotated_token for annotated_token in annotated_tokens if annotated_token]\n",
    "    annotated_tokens = [_chain_annotations(sentence) for sentence in annotated_tokens]\n",
    "    return annotated_tokens\n",
    "        \n",
    "def _chain_annotations(sentence: list):\n",
    "    annotation_heads = [token.xpath(\"./ann[@head]\") for token in sentence]\n",
    "    for token in annotation_heads:\n",
    "        for annotation_head in token:\n",
    "            annotation_channel = annotation_head.xpath(\"./text()\")[0]\n",
    "            annotation_type = annotation_head.get(\"chan\")\n",
    "            annotation_tokens = [token.xpath(\"./lex/base/text()\")[0] for token in sentence if token.xpath(f\"./ann[text()={annotation_channel}]\")]\n",
    "    return annotation_type, annotation_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a2a6d-d2b5-4cc1-8a3d-6e36e7ec58bb",
   "metadata": {},
   "source": [
    "The next cell demonstrates how to count tokens by comparing the number of tokens in the outputs for raw and spellchecked resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692ef11-0e3d-4e15-bd30-b84dfbfe271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Let's invastigate difference in number of parsed tokens in raw and spellchecked data\n",
    "\"\"\"\n",
    "\n",
    "# Raw\n",
    "token_nb_raw = 0\n",
    "for o in output_files_raw_fixed:\n",
    "    print(f\"Processing {o}\")\n",
    "    xml_tree = etree.parse(o)\n",
    "    token_nb_raw += count_tokens(xml_tree)\n",
    "print(f\"Raw data has {token_nb_raw} tokens\")\n",
    "\n",
    "# Spellchecked\n",
    "token_nb_spellchecked = 0\n",
    "for o in output_files_spellchecked_fixed:\n",
    "    print(f\"Processing {o}\")\n",
    "    xml_tree = etree.parse(o)\n",
    "    token_nb_spellchecked += count_tokens(xml_tree)\n",
    "print(f\"Spellchecked data has {token_nb_spellchecked} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c2a53-77a6-4469-9071-279038722a60",
   "metadata": {},
   "source": [
    "And now, a demonstration of how to get the most common annotations (= named entities) out of the output. The function that we have made to count annotations returns a [Counter](https://www.guru99.com/python-counter-collections-example.html), which is a very handy data structure that allows for comparison between different instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356bb67-42d2-486c-855a-8b49e9f7b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Let's check how annotation counts differ on entire output\n",
    "\"\"\"\n",
    "# Raw\n",
    "annotation_nb_raw = Counter()\n",
    "for o in output_files_raw_fixed:\n",
    "    print(f\"Processing {o}\")\n",
    "    xml_tree = etree.parse(o)\n",
    "    annotation_nb_raw += count_annotations(xml_tree)\n",
    "print(f\"Raw data annotations: {annotation_nb_raw.most_common(10)}\")\n",
    "\n",
    "# Spellchecked\n",
    "annotation_nb_spellchecked = Counter()\n",
    "for o in output_files_spellchecked_fixed:\n",
    "    print(f\"Processing {o}\")\n",
    "    xml_tree = etree.parse(o)\n",
    "    annotation_nb_spellchecked += count_annotations(xml_tree)\n",
    "print(f\"Spellchecked data annotations: {annotation_nb_spellchecked.most_common(10)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67747a88-d9c3-46aa-ac7c-c41e81bb108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Now, your job is to investigate how annotations number differs per file \n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "def align_resources(resources_raw, resources_spelled):\n",
    "    ret = []\n",
    "    for rraw in resources_raw:\n",
    "        for rspelled in resources_spelled:\n",
    "            if os.path.basename(rraw).split(\".\")[0] == os.path.basename(rspelled).split(\".\")[0]:\n",
    "                ret.append((rraw, rspelled))\n",
    "    return ret\n",
    "\n",
    "# We will obtain an array of tuples (raw, spellchecked), i.e. a Counter for each version that we can compare\n",
    "aligned_resources = align_resources(output_files_raw_fixed, output_files_spellchecked_fixed)\n",
    "\n",
    "\"\"\"\n",
    "    Fill in for loop body\n",
    "\"\"\"\n",
    "\n",
    "diff_counts: List[int] = []\n",
    "\n",
    "for rraw, rspelled in aligned_resources:\n",
    "    # Sum all differences in number of annotation occurences. Note that \n",
    "    # a) you can use `+` and `-` on Counter instance\n",
    "    # b) Counter instance does not store negative values (<0 are discarded from dict), \n",
    "    #    you can try: \n",
    "    #        diff_in_occur = sum(((counterA - counterB) + (counterB - counterA)).values())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86494369-2222-44f7-b5f7-b6fa79a73c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Plot the differences\n",
    "\"\"\"\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "resource_names = [os.path.basename(rraw).replace(\"_raw\", \"\") for rraw, _ in aligned_resources]\n",
    "plt.xticks(rotation=90)\n",
    "ax.bar(resource_names, diff_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a80eb-b107-453e-96b8-b7ae65673723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    SANDBOX\n",
    "    \n",
    "    Congratulations, you have finished our tutorial. Liner2 models come with different level of granularity of annotations. \n",
    "    Here, we invite you to investigate n82, that introduces additional level of granularity to annatations we presented \n",
    "    in previous sections\n",
    "\"\"\"\n",
    "predefined_resource = predefined_result[0]\n",
    "\n",
    "lpmn_client_task([predefined_resource], 'any2txt|wcrft2|liner2({\"model\":\"n82\"})', [f\"{predefined_resource}_n82\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
